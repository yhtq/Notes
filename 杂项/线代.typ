#import "@preview/touying:0.6.1": *
#import themes.university: *
#import "../template.typ": *
#show: note.with( 
  withOutlined: false,
  withChapterNewPage: false,
  withHeadingNumbering: false,
  withTitle: false
)
#show: university-theme.with(
  aspect-ratio: "16-9",
  // align: horizon,
  // config-common(handout: true),
  config-common(),  // freeze theorem counter for animation
  config-info(
    title: [线性代数],
    author: [郭子荀 2100012990\@stu.pku.edu.cn],
    date: datetime.today(),
    institution: [北京大学数学科学学院],

  ),
)
#set text(size: 22pt)
// #set heading(numbering: "1.")

#title-slide()

= 矩阵/线性方程组/线性空间
  == 矩阵
    - 矩阵是一个二维数组，可以进行加法、数乘、矩阵乘法等运算。
      #footnote[好像有个经典争议性问题是线性代数到底应该从矩阵/线性方程组/行列式/线性空间之中哪个开始讲起 :-) 所幸大家应该不是第一次学就从矩阵开始应该比较简洁]
    - 加法和数乘相对比较无趣，但乘法蕴含的信息比较丰富。典型的，矩阵乘向量#footnote[若无特殊说明我们说的向量都是列向量]：
      $
        mat(a_(11), a_(12); a_(21), a_(22)) vec(x_1, x_2) = vec(a_(11) x_1 + a_(12) x_2, a_(21) x_1 + a_(22) x_2)
      $
      它是矩阵乘法的起点。矩阵乘矩阵就可以定义为：
      $
        A (alpha_1, alpha_2, ..., alpha_n) = (A alpha_1, A alpha_2, ..., A alpha_n)
      $
      这里的记号为什么正确一定要非常熟悉，之后会非常频繁的用到。
  == 线性方程组
    用矩阵的语言，线性方程组就可以非常简洁的表示为：
    $
      A vec(x_1, x_2, dots.v, x_n) = vec(b_1, b_2, dots.v, b_m)
    $
    所有 $x$ 的可能取值就是方程组的解。
  == 线性方程组
    事实上，矩阵的每一行就是方程组的每一个方程。上面的等式也可以写作：
    $
      vec(a_1^T, a_1^2, dots.v, a_m^T) vec(x_1, x_2, dots.v, x_n) = vec(b_1, b_2, dots.v, b_m)\
      <=> forall i, a_i^T vec(x_1, x_2, dots.v, x_n) = b_i
    $
    （矩阵乘法的规则）
  == 线性空间
    线性空间应该是这几个概念之中比较抽象的一个。简单来说，线性空间是一些元素的集合，允许这些元素在空间内进行加法和数乘（与具体的数域有关）的操作，并且满足一些通常的运算公理。

    与此同时，保持线性运算的映射被称为线性映射，也就是满足：
    $
      forall u, v in V, alpha, beta in F,\
      T(alpha u + beta v) = alpha T(u) + beta T(v)
    $
    很明显，线性映射是一种特殊的映射，它的复合等等操作与普通的映射是一致的。同时，由于线性映射上定义加法，数乘都很平凡，因此线性映射的集合本身也是一个线性空间。之后如有用到，会记作 $Hom(L, V)$，表示从线性空间 $L$ 到线性空间 $V$ 的所有线性映射的集合。
  == 线性表示
    在任何线性空间（未必是 $F^n$）中，一组向量 $alpha_1, alpha_2, ..., alpha_n$ 的线性组合：
    $
      beta = k_1 alpha_1 + k_2 alpha_2 + ... + k_n alpha_n
    $
    也是一个向量。从直观的运算上，我们可以将其表示为：
    $
      beta = (alpha_1, alpha_2, ..., alpha_n) vec(k_1, k_2, dots.v, k_n) 
    $#footnote[这里的建议是不要试图理解/展开 $alpha$ 本身的含义，只把它当作形式上的运算就可以]
    对于一组表示系数，当然就可以得到一组线性组合：
    $
      (beta_1, beta_2, ..., beta_m) = (alpha_1, alpha_2, ..., alpha_n) mat(k_(11), k_(12), ..., k_(1 m); k_(21), k_(22), ..., k_(2 m); dots.v, dots.v, dots.down, dots.v; k_(n 1), k_(n 2), ..., k_(n m))
    $
    有趣的是，这里的 $k_(i j)$ 确实是一个矩阵，因此上式就可以记作：
    $
      (beta_1, beta_2, ..., beta_m) = (alpha_1, alpha_2, ..., alpha_n) K
    $
    请注意，这里的 $K$ 只是一个 $K$ 上的数表，与线性空间上的线性映射（暂时）没有任何关系。

    如果一组向量可以用非零系数线性表出零向量，则称它们线性相关，否则称它们线性无关。直观上，线性相关说明向量组是有冗余的。
  == 线性空间的基 
    一个极其重要的结论是，线性空间总是有基的。同时，对于有限维的线性空间（有一组有限的基），还有以下好的结论：
    - 所有基的元素个数相同，称为线性空间的维度。
    - 假设 $autoRVecN(alpha, n)$ 是一组基，则任何向量 $beta$ 都可以被唯一地线性表出，也就是：
      $
        exists! X in F^n. space beta = (alpha_1, alpha_2, ..., alpha_n) X
      $
      （这条实际上是基的定义）#footnote[这里的 $X$ 就是我们通常所说的坐标，它是 $F^n$ 中的一个向量，并非原线性空间 $V$ 中的向量]
    - 作为推论，设 $autoRVecN(alpha, n), autoRVecN(beta, n)$ 都是基，则它们互相线性表出，也就是存在 $P, Q$ 是 $F$ 上的矩阵，使得：
      $
        (beta_1, beta_2, ..., beta_n) = (alpha_1, alpha_2, ..., alpha_n) P\
        (alpha_1, alpha_2, ..., alpha_n) = (beta_1, beta_2, ..., beta_n) Q
      $
      事实上，此时总有 $P Q = Q P = I$
  == 线性映射是矩阵
    设 $V, W$ 是 $F$ 上的线性空间，且分别有基 $autoRVecN(alpha, n), autoRVecN(beta, m)$。则对 $V$ 中任何一个向量，可以将其写作：
    $
      v = (alpha_1, alpha_2, ..., alpha_n) X
    $
    使用 $T$ 作用后，就有：
    $
      T v = T ((alpha_1, alpha_2, ..., alpha_n) X)\
      = (T alpha_1, T alpha_2, ..., T alpha_n) X
    $
    而由于 $(T alpha_1, T alpha_2, ..., T alpha_n)$ 应该是 $W$ 中的向量组，因此可以被基 $autoRVecN(beta, m)$ 线性表出，继而可设：
    $
      (T alpha_1, T alpha_2, ..., T alpha_n) = (beta_1, beta_2, ..., beta_m) A
    $
    因此：
    $
      T v = (beta_1, beta_2, ..., beta_m) A X
    $
    显然，$A X$ 就是 $T v$ 这个向量的新的坐标。从上面的过程可以看出，$A$ 仅依赖于基的选取，与具体的向量和 $T$ 无关。

    这也是为什么，我们常说线性空间的线性映射可以用矩阵来表示的原因。但是，个人建议是不要滥用这个性质，因为在绝大多数时候把线性空间上的问题具体到矩阵上研究只会引入不必要的噪音。
  == 相似关系
    显然，相同的线性映射在不同的基下会有不同的矩阵表示。现在设 $T : V -> V$ 是 $V$ 上的线性变换，$autoRVecN(alpha, n), autoRVecN(alpha', n)$ 是 $V$ 的两组基，它们对应的 $T$ 矩阵分别为 $A, B$，则有：
    $
      (alpha_1, alpha_2, ..., alpha_n) A = T (alpha_1, alpha_2, ..., alpha_n)\
      (alpha'_1, alpha'_2, ..., alpha'_n) B = T (alpha'_1, alpha'_2, ..., alpha'_n)
    $
    如果假设：
    $
      (alpha'_1, alpha'_2, ..., alpha'_n) = (alpha_1, alpha_2, ..., alpha_n) P
    $
    代入立刻有：
    $
      (alpha_1, alpha_2, ..., alpha_n) P B = (alpha'_1, alpha'_2, ..., alpha'_n) B \
      = T (alpha_1, alpha_2, ..., alpha_n) P = (alpha_1, alpha_2, ..., alpha_n) A P
    $
    也就是 $P B = A P$。因此，我们把 $A, B$ 称为相似矩阵，如果存在可逆矩阵 $P$ 使得 $B = P^(-1) A P$。
  == 矩阵是线性映射
    另一方面，矩阵的乘法是自然的线性映射，也就是说，设 $A$ 是 $m times n$ 的矩阵，则它定义了从 $F^n$ 到 $F^m$ 的线性映射：
    $      T_A : F^n -> F^m,\
      T_A (X) = A X
    $
    在不引起混淆的情况下，我们将这个映射往往也记作 $A$，不过希望大家能够准确的意识到自己是在考虑一个矩阵还是在考虑一个线性映射。

    在这个意义下，矩阵的乘法和线性映射的复合是一致的，也即有：
    $
      T_(A B) = T_A compose T_B
    $
    （建议仔细验证一下这个结论）

    根据上节的讨论，可以证明 $F^n$ 到 $F^m$ 的所有线性映射恰好对应于所有的 $m times n$ 矩阵（留作练习#footnote[提示：取形如 $(0, 0, 0, dots, 1, 0, dots, 0)^T$ 这样的向量作为标准基，仔细验证上节讨论的所有运算，它们基本上都是直白的。]）
= 线性映射
  == 核与像
    设 $T : V -> W$ 是 $F$ 上的线性空间之间的线性映射，则称：
    - 核：$ker(T) = {v in V | T v = 0}$
    - 像：$im(T) = {w in W | exists v in V, T v = w}$
    可以验证，核与像都是各自空间的子空间。核描述了哪些向量被映射为零，而像描述了哪些向量可以被映射到。我们有以下的性质：
    - $T$ 单射的充分必要条件是 $ker(T) = {0}$
    - $T$ 满射的充分必要条件是 $im(T) = W$
    - $im T eqv V quo (ker T)$
    - $dim ker T + dim im T = dim V$
    - 特别的，若 $T$ 是双射，则 $dim V = dim W$
  == 列空间/解空间
    之前提到，对于一组向量 $autoRVecN(alpha, n)$，我们可以把它形式的记作向量组成的行向量 $autoRVecN(alpha, n)$. 特别的，如果这些向量就是 $F^m$ 中的向量（恰有 $m$ 行），则它们组成的行向量恰好就是 $m times n$ 的矩阵：
    $
      A = (alpha_1, alpha_2, ..., alpha_n)
    $
    可以发现，它的列空间（即所有列向量生成的子空间，或者说所有列向量的任何线性组合）恰好就是把 $A$ 看作线性映射 $x |-> A x$ 时的像空间。

    设 $A$ 是 $m times n$ 矩阵，则线性方程组 $A x = 0$ 的所有解组成的集合称为解空间。可以发现，解空间恰好就是线性映射 $x |-> A x$ 的核空间。
  == 秩
    我们称一个线性映射的秩就是它像空间的维度 $dim im(T)$，我们有以下基本是由定义显然的性质：
    - 满射的秩就是 $dim W$，也即目标空间的维度
    - 线性映射的秩就是：
      - 对应矩阵的列空间的维度
      - 对应矩阵的列秩
      - 对应矩阵的列向量组的极大线性无关组的大小#footnote[事实上，极大线性无关组就是这些向量张成空间的一组基]
    - $rank(P T Q) = rank(T)$，其中 $P, Q$ 是可逆的线性映射（对应矩阵的左乘/右乘可逆矩阵）
    还有以下需要一些巧妙证明的性质：
    - $rank(A) = rank(A^T) = rank(A A^T) = rank(A^T A)$
    - $rank(A + B) <= rank(A) + rank(B)$
    - $rank(A B) <= min(rank(A), rank(B))$
    - 若方阵的行列式非零，则矩阵满秩
  == 可逆性
    可逆的线性映射称为同构。我们有如下的等价条件：
    - $T$ 是同构
    - $T$ 是双射
    - $ker(T) = {0}$ 且 $im(T) = W$
    - $dim V = dim W$ 且 $rank(T) = dim V$
    - 在某组基下的对应矩阵 $A$ 可逆
    - $im T$ 中包含一组 $W$ 的基
    还有一些典型的推论，包括：
    - 矩阵可逆等价于对应的线性映射可逆（提示：矩阵乘法就是线性映射的复合）。
    - 可逆矩阵的逆矩阵也是可逆矩阵。
= 双线性映射与二次型
  == 双线性映射
    设 $V, W, U$ 是 $F$ 上的线性空间，则称二元映射 $B : V times W -> U$ 是双线性的，若对于任意固定的 $v in V$，映射 $B(v, -) : W -> U$ 是线性映射，且对于任意固定的 $w in W$，映射 $B(-, w) : V -> U$ 是线性映射。

    简单来说，双线性映射就是对两个变量都线性的映射。大家熟悉的点积就是典型的双线性映射。
  == 矩阵表示
    类似于之前我们做的事情，可以把一个双线性函数 $B : V times W -> F$ 用矩阵表示出来。设 $autoRVecN(alpha, n), autoRVecN(beta, m)$ 分别是 $V, W$ 的基，则对于任意 $v in V, w in W$，有：
    $
      v = (alpha_1, alpha_2, ..., alpha_n) X,\
      w = (beta_1, beta_2, ..., beta_m) Y\
      B (v, w) = B((alpha_1, alpha_2, ..., alpha_n) X, (beta_1, beta_2, ..., beta_m) Y)\
      = X^T (B(alpha_i, beta_j))_(i, j) Y
    $
    中间 $(B(alpha_i, beta_j))_(i, j)$ 自然就是我们要找的表示。反之，给定一个矩阵 $C$，我们也可以定义一个双线性映射：
    $
      (X, Y) |-> X^T C Y
    $
  == 二次型的合同
    类似的，我们可以寻找不同基下，相同的二次型的矩阵表示之间的关系。这里我们略过推导，结论是，设 $Q : V -> F$ 是二次型，$autoRVecN(alpha, n), autoRVecN(alpha', n)$ 是 $V$ 的两组基，它们对应的 $Q$ 矩阵分别为 $A, B$，则有：
    $
      B = C^T A C
    $
    其中 $C$ 是某个可逆矩阵
= 行列变换
  == 初等变换
    大家都知道，对矩阵可以做一些初等的行列变换使得某些性质被保持，包括：
    - 交换两行/列
    - 某一行/列乘以非零常数
    - 某一行/列加上另一行/列的若干倍
    大量的实际计算都基于这些初等变换。对于实际的计算来说，个人认为虽然可能有一些小窍门，但绝大多数时候思考小窍门都只是浪费感情））使用最标准的，逐步将每行或者每列的元素归零就足够解决几乎所有问题了（尽管看上去可能复杂，但做起来未必复杂）
  == 初等矩阵
    事实上，每一个初等变换都可以用一个初等矩阵来表示。一个行变换总是对应一个初等矩阵的左乘，而列变换总是对应一个初等矩阵的右乘。具体的对应关系建议大家需要的时候再对照写一写就好。

    前面说的许多内容实际上都需要通过初等变换计算，包括：
    - 求行列式（$det(P A Q) = det(A)$）
    - 求秩（$rank(P A Q) = rank(A)$）
    - 求齐次线性方程组的解（$A x = 0 <=> P A x = 0$）
    - 求列空间（$im A = im A Q$）
    - 求逆（$P A = I => Inv(A) = P$）
    相信大家作业中肯定都遇到过，这些计算都需要很熟练，并且理解为什么是正确的。
  == 相抵关系
    我们称两个矩阵 $A, B$ 是相抵的，若存在可逆矩阵 $P, Q$ 使得 $B = P A Q$。可以证明，任何矩阵总可以写成：
    $
      A = P mat(I_r, 0; 0, 0) Q
    $
    其中 $r = rank(A)$，且 $P, Q$ 可逆矩阵。因此，矩阵的相抵类完全由秩决定。具体计算上，它就是不断的使用行列变换，把矩阵化为标准形即可。
= 特征值、特征向量、对角化
  == 特征值与特征向量
    设 $T : V -> V$ 是 $F$ 上的线性空间 $V$ 上的线性映射，则称 $lambda in F$ 是 $T$ 的特征值，若存在非零向量 $v in V$ 使得：
    $
      T v = lambda v
    $
    此时，称 $v$ 是对应于特征值 $lambda$ 的特征向量。

    换句话说，特征值就是使得线性映射在某个非零向量上表现为数乘的那些数，而这些非零向量就是对应的特征向量。

    对于矩阵，它的特征值与特征向量就是把矩阵看作线性映射后的特征值与特征向量。
  == 特征值的计算
    设 $A$ 是矩阵，则不难看出 $lambda$ 是特征值当且仅当 $(T - lambda I) X = 0$ 作为线性方程组有解，也就是：
    $
      det(T - lambda I) = 0
    $
    同时，上式是一个关于 $lambda$ 的 $n$ 次多项式方程，称为特征多项式。解出这个多项式的根就可以得到所有的特征值。显然，特征值和特征向量的状况是与数域的选取有关的。
  == 对角化
    典型的应用是，假设 $A$ 有 $n$ 个线性无关的特征向量 $autoRVecN(v, n)$，对应的特征值为 $autoRVecN(lambda, n)$，则我们可以写出：
    $
      A autoRVecN(v, n) = (lambda_1 v_1, lambda_2 v_2, ..., lambda_n v_n) = autoRVecN(v, n) mat(lambda_1, 0, ..., 0; 0, lambda_2, ..., 0; dots.v, dots.v, dots.v, dots.v; 0, 0, ..., lambda_n)
    $
    若设 $V = autoRVecN(v, n)$，上式立刻就是：
    $
      A = V mat(lambda_1, 0, ..., 0; 0, lambda_2, ..., 0; dots.v, dots.v, dots.v, dots.v; 0, 0, ..., lambda_n) Inv(V)
    $
    事实上，不同特征值对应的特征向量总是线性无关的，因此只要特征多项式有 $n$ 个不同的根，矩阵就一定可以对角化。
  == 实对称矩阵的对角化
    可以证明。设 $A$ 是实对称矩阵，则它的所有特征值都是实的（建议复习证明），同时，存在正交矩阵 $P$ 使得：
    $
      A = P D Inv(P)
    $
    其中 $D$ 是对角矩阵，且对角线上的元素就是 $A$ 的特征值。正交矩阵是指满足 $P^T P = I$ 的矩阵。这种情况下，不同特征向量之间不仅是线性无关的，甚至是正交的。
  == 正交矩阵
    正交矩阵是内积的线性变换。具体来说，设 $P$ 是 $R^n$ 上的变换，满足：
    $
      forall x, y in R^n, (P x)^T (P y) = x^T y
    $
    则称 $P$ 是正交变换。可以证明，正交变换一定是线性的，并且它对应的矩阵满足 $P^T P = I$。反之，满足 $P^T P = I$ 的矩阵一定是正交变换。

    等价的，正交矩阵可以理解为：
    - 标准正交基到标准正交基的变换矩阵
    - 标准正交基作为列组成的矩阵
    - 标准正交基作为行组成的矩阵
    - 保持向量长度的线性变换，也即：
      $
        forall x in R^n, (P x)^T (P x) = x^T x
      $
= 对称二次型的标准型
  == 同时行列变换
    设 $A$ 是一个对称矩阵，为了在合同意义下化简它，可以进行同时的行列变换，也就是说，对 $A$ 不断地在行与列上做相同的初等变换。设 $P$ 是一个可逆矩阵，则对 $A$ 进行 $P$ 的行列变换后得到的矩阵为：
    $
      B = P^T A P
    $
  == 对称标准型
    由于实对称矩阵可以对角化，因此我们可以把 $A$ 化为对角矩阵 $D$：
    $
      A = Q D Q^T
    $
    而至于 $D$ 的对角元素，我们可以通过进一步的初等变换把它们化为 $1, -1, 0$（这里是因为，同时行列变换要求我们在行与列上同时乘某个元素，因此作用到对角线上，必须就是乘这个元素的平方），众所周知，实数域上找不到 $-1$ 的平方根，因此无法把 $-1$ 化为 $1$。最终，我们可以把 $A$ 化为如下形式：
    $      A = P^T mat(I_r, 0, 0; 0, -I_s, 0; 0, 0, 0) P
    $
    其中 $r, s$ 分别是正惯性指数与负惯性指数。可以证明，惯性指数与选取的初等变换无关，因此它们是矩阵的合同不变量。
  == 正定矩阵
    设 $A$ 是实对称矩阵，则称 $A$ 是正定的，如果以下（等价的）条件之一成立：
    - 对任意非零向量 $X$，都有 $X^T A X > 0$
    - $A$ 的所有特征值均为正实数
    - $A$ 的正惯性指数为 $n$（矩阵大小）
    - $A$ 的所有顺序主子式均大于零
    - $A$ 合同于单位矩阵
    - $A$ 可以写成 $C^T C$ 的形式，且 $C$ 可逆
    正定矩阵的概念连接了线性代数的诸多方面，在实践中应用非常广泛（同时也有很多的题可出(:- ） 解题的关键往往是在以上等价的的条件之间来回转换。
= 杂项
  == 线性子空间的交
    设 $V$ 是 $F$ 上的线性空间，$W_1, W_2$ 是 $V$ 的两个子空间，则它们的交 $W_1 inter W_2$ 也是 $V$ 的子空间。假设给定两组向量 $autoRVecN(alpha, n)$ 和 $autoRVecN(beta, m)$，分别生成子空间 $W_1, W_2$，则可以通过如下的方式计算它们的交：
    - 假设 $X$ 在子空间的交中，则：
      $
        exists A in F^n, B in F^m,\
        X = (alpha_1, alpha_2, ..., alpha_n) A = (beta_1, beta_2, ..., beta_m) B
      $
      进而有：
      $
        (alpha_1, alpha_2, ..., alpha_n, beta_1, beta_2, ..., beta_m) mat(A;-B) = 0
      $
      关于 $A, -B$，这是 $n + m$ 元的线性方程组，解出所有解之后，取所有前 $n$ 个元素作为 $alpha$ 下的表出系数即可。
  == 零化多项式
    设 $f$ 是一多项式，若 $f(A) = 0$，则称它是矩阵 $A$ 的零化多项式。零化多项式与矩阵的相似标准型等问题有着深刻的关系，不过这里只介绍一些简单情景。例如，假设 $f = (x - a)(x - b)$ 且 $A != B$，对应的，就是说：
    $
      0 = f(A) = (A - a I)(A - b I)
    $
    可以断言，$A$ 一定是可对角化的。事实上，可以证明：
    $
      ker(A - a I) directSum ker(A - b I) = F^n
    $<39>
    - 首先，$ker(A - a I) + ker(A - b I) = F^n$，这是因为任取向量 $X$ 总有：
      $
        X = 1/(b - a) ((A - a I) X - (A - b I) X) 
      $
      这里，就有：
      $
        (A - a I) X in ker (A - b I),\
        (A - b I) X in ker (A - a I)
      $
    - 其次，$ker(A - a I) inter ker(A - b I) = {0}$，这是因为若 $X$ 在交中，则有：
      $
        (A - a I) X = 0,\
        (A - b I) X = 0
      $
      这表明：
      $
        A X = a X = b X
      $
      只要 $X$ 非零，就有 $a != b$，矛盾
    综上，@39 表明 $A$ 的 $a$ 特征向量和 $b$ 特征向量张成整个空间，因此 $A$ 可对角化。




