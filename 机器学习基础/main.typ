#import "../template.typ": *
#show: note.with(
  title: "机器学习基础",
  author: "YHTQ",
  date: datetime.today().display(),
  logo: none,
  withChapterNewPage: true
)
#let ge(h) = $R(#h)$
#let ee(h) = $hat(R)(#h)$
#let er(h) = $hat(e)_r (#h)$
#let acc(h) = $hat(a) (#h)$
= 前言  
  - 教材：周志华《机器学习》
  - 期末考试：6.10 下午，闭卷笔试
  - 作业包括纸面作业和上机作业，都在教学网进行
  - 作业/考勤 40% + 期中 20% + 期末 40%
  - 教学大纲：
    + 机器学习概论
    + 计算学习理论
    + 机器学习基本模型和算法
    + 深度学习初步
= 基本概念
  == 监督学习
    所谓监督学习，就是基于一组数据 $T = {(x_i, y_i)}$ 来学习输入空间 $X$ 到输出空间 $Y$ 的映射。通常认为样本是独立同分布的。$y_i = c(x_i)$ 有时（$c$）也称为目标概念。所有概念记为 $C$，而所有可能的概念一般记为假设空间 $H$，往往与 $C$ 不同。学习算法基于 $T$ 得到一个假设 $h_T in H$
    #definition[损失函数][
      用 $L(h(x), y)$ 度量以 $h(x)$ 作为预测的预测好坏，常见的包括：
      - 0-1 损失
        $
          L(h(x), y) = delta_(h(x), y)
        $ 
      - 平方损失
        $
          L(h(x), y) = (h(x) - y)^2
        $
    ]
    #definition[泛化误差][
      给定损失函数，则：
      $
        ge(h) = E_(x tilde D) L(h(x), c(x))
      $
      表明平均意义下预测的好坏，称其为泛化误差
    ]
    注意通常来说，$x$ 的分布 $D$ 和真实概念 $c$ 都是未知的，因此泛化误差通常是无法实际计算的。
    #definition[经验误差/测试误差][
      - 给定数据集 $T$，则：
        $
          ee(H) = E_T L(h(x), y_i) = 1/N sum_i L(h(x_i), y_i)
        $
        称为经验误差。经验误差小未必保证泛化误差小，可能产生*过拟合*现象。

        更进一步，不同的机器学习理论可能给出对：
        $
          P(abs(approxVar(R)(h_T) - R(h_T)) < epsilon) 
        $
        的估计
      - 类似的，在测试集上计算的误差称为测试误差。通常来说，测试集的产生与训练过程无关，可以认为它们独立，同时有：
        $
          E (approxVar(R)_"test" (h_T)) = R(h_T) 
        $

    ]
    #definition[误差率/准确率][
      用：
      $
        er(h_T) = E_T delta_(h(x), y_i)
      $
      表示算法在 $T$ 上的误差率。用：
      $
        acc(h_T) = 1 - er(h_T)
      $
      表示算法在 $T$ 上的准确率。
    ]
    经验误差小并不能保证泛化误差组功效。
    
    实际二分类问题中，往往会将结果分为真/T 以及假/F，因此产生四类情况：
    - TP：真正例
    - FP：假正例
    - TN：真负例
    - FN：假负例
    通常：
    - $"TP"/("TF" + "TP")$ 称为准确率/查准率
    - $"TP"/("TP" + "FN")$ 称为召回率/查全率

    实际训练模型中，通常来说有两种策略：
    - 经验风险最小化：最小化经验误差
    - 结构风险最小化/正则化策略：
      $
        argmin_h approxVar(R) (h) + lambda J(h)
      $
      其中 $J(h)$ 为正则化项，$lambda$ 为正则化参数。实践上，这两者的选择方法是十分复杂的。
    
    实践上由于超参数的存在，数据往往还要分出一部分用以验证，具体来说：
    - 选取不同的超参数，在训练集上训练
    - 在验证集上验证，选取最优的超参数
    - 在测试集上测试，得到最终的对模型的评价
    #algorithm[数据集的划分][
      - 留出法/简单交叉验证法：通过随机采样，将数据划分为训练集和测试集。有时可以采用分层采样的方式，保证数据的分布。
      - $k$ 值交叉验证法：将数据划分为 $k$ 个大小相等的子集，每次取一个子集作为测试集，其余的作为训练集，重复 $k$ 次，得到 $k$ 个模型，取平均值。
      - 留一法：$k = N$ 的特殊情况，每次只取一个样本作为测试集，其余的作为训练集。
      - 自助法：通过有放回的采样，得到一定大小的训练集，剩余的作为测试集。这样采样可以保证验证集，训练集的大小相同，但得到的分布往往和原分布有所不同。
    ]

    #definition[][
      - 定义：
        $
          overline(h) (x) = E_T h_T (x)\
          "Bias" (x) = E_T (h_T (x) - c(x)) \
          "Var" (x) = E_T (h_T (x) - overline(h) (x))^2
        $
    ]
    #lemma[][
      $
        E_Y (h_T (x) - c(x))^2 = "Bias" (x)^2 + "Var" (x)
      $
    ]
    然而，实际上偏差小（表示对数据敏感）和方差小（对数据不敏感）往往是矛盾的。
= 支持向量机
  以二分类任务为例，使用最简单的线性模型，就是找一个超平面将数据分开：
  - 若假设存在这样一个超平面，如何找到？
  - 若不假设，如何找到一个最优的超平面？
  == 线性可分支持向量机
    给定一个线性可分的数据集 $T = {(x_i, y_i)}$，其中 $y_i in {1, -1}$，则存在一个超平面 $w^T x + b = 0$，使得对所有的 $i$ 有 $y_i (w^T x_i + b) > 0$。

    若模型正确分类，则一定有：
    $
      y_i (w^T x_i + b) > 0, forall i
    $
    为了找到最好的超平面：
    - 由齐次性，不妨假设：
      $
        abs(w x_i + b) >- 1
      $
    - 特别的，对于 $y_i (w x_i + b) = 1$ 的样本点而言，它们就是距离超平面最近的点，而一对这样的点的距离恰好为 $1/norm(w)$
    - 我们把 $2/norm(w)$ 称为*间隔*，目标即是找到间隔最大的超平面
    因此，我们得到了优化问题：
    $
      max_(w, b) 1/(norm(w)) suchThat y_i (w^T x_i + b) > 0, forall i 
    $
    其中 $1/(norm(w))$ 为间隔，表示超平面到最近的点的距离。它可以转化为：
    $
      min_(w, b) 1/2 norm(w)^2 suchThat y_i (w^T x_i + b) > 0, forall i
    $
    这是凸二次优化问题，只要有解（原问题线性可分），则解唯一。称它的解为最大间隔分离超平面。某种意义上，就是利用下面的神经元：
    $
      f(x) = sgn(w^T x + b)
    $
    为了求解这个问题，构造拉格朗日函数：
    $
      L = 1/2 norm(w)^2 - sum_(i = 1)^N alpha_i - sum_(i = 1)^N alpha_i y_i (w^T x_i + b)
    $
    可得：
    $
      W = sum_i alpha_i y_i x_i\
      sum_i alpha_i y_i = 0
    $
    显然 $W != 0$，因此 $alpha_i$ 不能全为零。可以证明：
    $
      L = sum_i alpha_i - 1/2 sum_(i, j) alpha_i alpha_j y_i y_j x_i^T x_j
    $
    因此原问题等价于：
    $
      min_alpha sum_i alpha_i - 1/2 sum_(i, j) alpha_i alpha_j y_i y_j x_i^T x_j \
      suchThat sum_i alpha_i y_i = 0, alpha_i >= 0
    $
    这个问题被称为原问题的对偶问题。

    由 KKT 条件，若 $alpha_i != 0$，则：
    $
      y_j (w^T x_j + b) = 1\
      b = y_j - w^T x_j = y_j - sum_(i = 1)^N alpha_i y_i x_i^T x_j
    $
    由上面的推导，往往也将支持向量机写作：
    $
      sgn(sum_i alpha_i y_i x_i^T x + b)
    $
    事实上，还可以得到：
    $
      norm(w)^2 = sum_(i, j) alpha_i alpha_j y_i y_j x_i^T x_j\
      = sum_j alpha_j y_j (y_j - b)\
      = sum_j alpha_j
    $
    因此，事实上我们希望不为零的 $alpha_i$ 尽可能少，这些向量就被称为支持向量。可以证明，假设在训练集 $D$ 上采用留一法，定义留一误差为：
    $
      ee(f) = 1/N sumBrN1(I(f_(D^(-i)) != y_i))
    $
    则有：
    $
      ee(f) <= 1/N N_("SV")(f)
    $
    其中 $N_("SV")(f)$ 是支持向量的个数。（该估计仅做了解）
  == 非线性可分支持向量机
    在大多数时候，要求线性可分有些严苛。此时，不仅要最小间隔尽可能大，还要希望分类错误的点尽可能少。因此，我们为每个点引入松弛量 $xi_i$，约束条件变为：
    $
      y_i (w x_i + b) + xi_i >= 1
    $
    我们将 $xi_i != 0$ 的点称为特异点，对于非特异点，情形与线性可分情形是相同的。为了区分其间，我们将此时的间隔称作软间隔，可分情形称作硬间隔。我们希望间隔尽可能小，并且特异点也要尽可能少，因此我们得到了优化问题：
    $
      min_(w, b) 1/2 norm(w)^2 + C sum_i 1_(xi_i !=0) \ 
      suchThat y_i (w x_i + b) >= 1 - xi_i, xi_i >= 0
    $
    实践上，往往采用以下的松弛版本：
    $
      min_(w, b) 1/2 norm(w)^2 + C sum_i xi_i\
      suchThat y_i (w x_i + b) >= 1 - xi_i, xi_i >= 0
    $<sv-2>
    有时也可采用 $C sum_i xi_i^p$ 作为松弛项。

    在上面的问题中，显然取 $xi_i = max(0, 1 - y_i (inner(w, x_i) + b))$ 即可。因此引入合页损失函数：
    $
      h(z) = max(0, 1 - z)
    $ 
    问题等价为
    $
      min_(w, b) 1/2 norm(w)^2 + C sum_i h(y_i (inner(w, x_i) + b))
    $
    或者：
    $
      min_(w, b) 1/2 sum_i max(0, 1 - y_i (inner(w, x_i) + b)) + 1/C norm(w)^2
    $
    事实上，这就是采取合页损失的结构风险最小化策略。事实上，之前的松弛就是用合页损失替代了 $0-1$ 损失。当然也可以采用二次损失替代合页损失。

    回到@sv-2，假如使用拉格朗日乘子法，可以得到拉格朗日函数：
    $
      L = 1/2 norm(w)^2 + C sum_i xi_i - sum_i alpha_i (y_i (inner(w, x_i) + b) - 1 + xi_i) - sum_i beta_i xi_i
    $
    求导得到：
    $
      w = sum_i alpha_i y_i x_i\
      sum_i alpha_i y_i = 0\
      alpha_i = C - beta_i
    $
    化简得到：
    $
      L = sum_i alpha_i - 1/2 sum_(i, j) alpha_i alpha_j y_i y_j inner(x_i, x_j)
    $
    因此得到对偶问题：
    $
      max_alpha sum_i alpha_i - 1/2 sum_(i, j) alpha_i alpha_j y_i y_j inner(x_i, x_j)\
      suchThat sum_i alpha_i y_i = 0, 0 <= alpha_i <= C
    $<sv-3>
    可以看到，形式上只是增加了约束 $alpha_i <= C$，同时许多结论仍然成立：
    - 仍然可以将 $alpha_j > 0$ 的点称为支持向量
    - $w = sum_i alpha_i y_i x_i$
    - $b = y_i - sum_j alpha_j y_j inner(x_j, x_i)$
    - 在所有的支持向量中，$0 < alpha_i < C$ 的点落在分类边界上，$alpha_i = C$ 的点是特异点。
    注意偏置 $b$ 往往不唯一，需要后续实验确认
  == SMO 算法
    Platt 提出的*序列最小最优化算法（SMO）*可以高效地解决@sv-3 这种二次规划问题。其基本思想是：
    - 将多个拉格朗日乘子的问题化减到仅有两个乘子的问题。往往，我们选择第一各变量是违反 KKT 条件最严重的，第二个变量是使得目标函数增加最快的。
    - 仅有两个乘子的二次规划问题有解析解
    首先考虑仅有两个乘子的问题求解：
    #algorithm[][
      假设当前只更新拉格朗日乘子 $alpha_1, alpha_2$，约束：
      $
        sum_i alpha_i y_i = 0
      $
      给出：
      $
        alpha_1 y_1 + alpha_2 y_2 = - sum_(i != 1, 2) alpha_i y_i\
        alpha_1 + alpha_2 y_1 y_2 = - y_1 sum_(i != 1, 2) alpha_i y_i
      $
      令 $s = y_1 y_2, gamma = - y_1 sum_(i != 1, 2) alpha_i y_i$，则原问题转化为
      $
        max_(alpha_1, alpha_2) W(alpha_1, alpha_2)\
        suchThat 0 <= alpha_1, alpha_2 <= C, alpha_1 + s alpha_2 = gamma
      $
      事实上，使用 $alpha_1 = gamma - s alpha_2$ 代换后，这就是关于 $alpha_2$ 的一元二次函数，可以直接求最大值。

      注意每次更新 $alpha_1, alpha_2$ 后，都需要更新 $b$. 若 $alpha_1, alpha_2$ 中至少一个非零，则应该采用 $b = y_j - inner(sum_i alpha_i y_i x_i, x_j)$ 更新
    ]
== 核方法 
  许多问题是完全不能依靠线性函数划分的。一个很自然的想法是构造映射 $phi$，使得 $phi(X)$ 线性可分。得到最优化问题：
  $
    min_(w, b, xi) 1/2 norm(w)^2 + C sum_i xi_i\
    suchThat y_i (inner(w, phi(x_i)) + b) >= 1 - xi_i, xi_i >= 0
  $
  或者对偶问题：
  $
    max_alpha sum_i alpha_i - 1/2 sum_(i, j) alpha_i alpha_j y_i y_j inner(phi(x_i), phi(x_j))\
    suchThat sum_i alpha_i y_i = 0, 0 <= alpha_i <= C
  $
  实际应用上，$phi$ 是难以构造的。经典的处理方式是引入*核函数*：观察上面的对偶问题，事实上我们不关系 $phi$ 的具体值，只关心：
  $
    K(x_1, x_2) = inner(phi(x_1), phi(x_2))
  $
  在数据集上的值，因此合理选择核函数即可，最终的分类函数形如：
  $
    f(x) = sgn(sum_i alpha_i y_i K(x_i, x) + b)\
    b = y_j - sum_i alpha_i y_i K(x_i, x_j)
  $
  往往我们会选择正定对称的核函数，常用的包括：
  - 线性核函数：$K(x, z) = inner(x, z)$
  - 多项式核函数：$K(x, z) = (inner(x, z) + 1)^p$
  - 高斯核函数：$K(x, z) = exp(-d(x, z)^2 / (2 sigma^2))$
= 基于近邻的分类方法
  这类方法又称为免模型方法或者惰性学习，最终得到的不是一个显式的模型，而是根据新样本落在哪一类作为预测结果。这类方法的核心问题包括：
  - 如何度量数据的相似性
  - 选择哪些与新数据相似的实例
  - 如何利用选定样本的类标记来预测新数据的类标记
  在特征空间为 $RR^n$ 的情况下，最常用的距离度量是 MinKowski 距离
= 决策树方法
  对于分类问题，我们也可以采用决策树方法，根据规则对数据进行分类。
  #definition[经验熵][
    定义训练集的经验熵如下：
    $
      H(D) = - sumBrN1(p_i log p_i) where p_i = abs(D_i)/abs(D)
    $
    它度量了数据集的“纯度”，经验熵越大，数据集的信息量越大。可以证明在 $p_i$ 都相等的时候，经验熵最大。
  ]
  #definition[][
    对于特征 $A$，其对数据集 $D$ 的经验条件熵定义为：
    $
      H(D|A) = sumBrN1(abs(D_i^A)/abs(D) H(D^A))
    $
    进一步，定义信息增益为：
    $
      G(D, A) = H(D) - H(D|A)
    $
  ]
  直观来说，我们应该向着信息增益越大的方向分类。然而，信息增益有一个缺点，它偏向于选择取值较多的特征。因此，我们可以引入校正项：
  #definition[][

  ]
  除了使用熵作为指标，还可以选择其他的指标：
  #definition[][
    定义基尼指数：
    $
      "Gini"(D) = 1 - sumBrN1(p_i^2)
    $
    它相当于从数据集中随机抽取两个样本，类别不一致的概率。基尼指数越小，数据集的纯度越高。

    类似的，定义特征 $A$ 对数据集 $D$ 的基尼指数：
    $
      "Gini"(D, A) = sumBrN1(abs(D_i^A)/abs(D) "Gini"(D^A))
    $
    它越小，表明特征 $A$ 的分类能力越强。
  ]
  选定一个指标 $F$，我们就可以设计一般的决策树算法：
  + 若样本集 $D$ 中所有元素属于同一类别 $C$，则返回单节点树 $T$
  + 若 $A = emptyset$，且 $D$ 非空，则生成以 $D$ 中样本数最多的类别 $C$ 为标记的节点
  + 否则，计算所有特征 $A$ 对数据集 $D$ 的度量，选择最优特征 $A_*$，划分数据集 $D$ 为 $D_1, D_2, ..., D_n$，对于每个 $D_i$ 递归生成决策树
  历史上，选择信息增益最大的算法称为 ID3 算法，选择信息增益比最大的算法称为 C4.5 算法，选择基尼指数最小的算法称为 CART 算法。注意 ID3, C4.5 算法中，我们对 $A$ 的每个取值做多路划分，而 CART 只对 $A = a_i, A eq.not a_i$ 做二路划分，选择特征时也要选择最优的划分点。

  然而，决策树也可能产生“过拟合”，因此我们希望对决策树进行剪枝，以达到某种结构风险最小。大致分为两种策略：
  - 预剪枝策略：在生成过程中，基于验证集对当前划分做出评估，如果泛化能力未提升则直接标记为叶节点。然而，这种方法相当于提前禁止一些分支展开，可能会导致欠拟合。
  - 后剪枝策略：生成完成后，对决策树进行剪枝，根据验证集比较剪枝后的树和原树的泛化能力，选择最优的树。这种方法相对来说更加保守，但是也更加稳定。
  同时，后剪枝策略也可以采取正则化方法。给定一个决策树 $T$，可以将损失函数定为：
  $
    C_alpha(T) = C(T) + alpha abs(T)
  $
  其中 $C(T)$ 是拟合程度，例如：
  $
    C(T) = sum_(t = 1)^abs(T) N_t H_t (T)
  $
  其中 $N_t$ 是叶节点 $t$ 的样本数，$H_t(T)$ 是叶节点 $t$ 的经验熵（事实上 $C(T)$ 接近于负的对数似然函数）。$alpha$ 是正则化参数，用以平衡拟合程度和模型复杂度。

  有时，我们也会遇到特征连续的数据。此时，常用策略是对于每个特征，列出出现在数据集中的所有情况，取所有中点作为划分点。
  == 最小二乘回归树
    有时，我们也可以使用决策树进行回归。对于回归问题，我们可以使用最小二乘回归树。其基本思想是将输出函数变成阶梯形：
    $
      f(x) = sum_(m = 1)^M c_m 1_(x in R_m)
    $
    其中 $R_m$ 是选定的单元，通常 $c_m$ 就是 $R_m$ 中所有数据的均值（这样可以达到最小的最小二乘误差）。只要选定 $R_m$，问题就变成了分类问题。具体操作如下：
    - 选择一个特征 $j$ 和一个切分点 $s$，将数据集划分为 $R_1(j, s) = {x|x_j <= s}$ 和 $R_2(j, s) = {x|x_j > s}$，使得：
      $
        min_(c_1) sum_(x_i in R_1) (y_i - c_1)^2 + min_(c_2) sum_(x_i in R_2) (y_i - c_2)^2
      $
      最小。（上式中 $c_1, c_2$ 就取 $R_1, R_2$ 中所有数据的均值）
    - 基于上面的 $j, s$，将数据集划分为 $R_1, R_2$，重复上述过程，直到达到停止条件。
    类似的，我们也可以仿照前面的思想进行剪枝。采用损失函数：
    $
      C_alpha (T) = C(T) + alpha abs(T)
    $
    其中：
    $
      C(T) = sum_(t = 1)^abs(T) N_i Q_i (T)\
      where Q_t (T) = 1/N_t sum_(x_i in R_t) (y_i - c_t)^2
    $
  == 小结
    - 决策树模型是基于特征对实例进行分类的树形结构。
    - 学习算法往往采用贪心策略
    - ID3 算法，C4.5 算法，CART 算法是常见的决策树学习算法，分别采用信息增益最大，信息增益比最大，基尼指数最小作为特征选择准则。
    - ID3 算法，C4.5 算法采用多路划分，CART 算法采用二路划分。
    信息熵、条件信息熵的计算需要掌握
= 神经网络学习初步
  == M-P 神经元模型，多层前馈神经网络
    前面的许多机器学习模型可以表达为线性模型和非线性模型的组合。也就是，先计算一个线性结果，再使用非线性的激活函数处理线性结果，这就是一个 M-P 神经元。
    #definition[Sigmoid][
      定义：
      $
        f(x) = 1/(1 + exp(-x))
      $
      为 Sigmoid 函数，它是一个很传统的激活函数。
    ]
    #definition[多层前馈神经网络][
      称：
      - 神经元逐层排列，相邻层神经元全连接，不相邻层无连接
      - 第一层为输入层，最后一层为输出层，中间层为隐藏层
      - 除输入层外，每层都通过激活函数处理输入
    ]
    通常而言，宽度和深度都能增加神经网络的表达能力，但深度往往更加经济。



