#import "../template.typ": *
#show: note.with(
  title: "机器学习基础",
  author: "YHTQ",
  date: datetime.today().display(),
  logo: none,
  withChapterNewPage: true
)
#let ge(h) = $R(#h)$
#let ee(h) = $hat(R)(h)$
#let er(h) = $hat(e)_r (#h)$
#let acc(h) = $hat(a) (#h)$
= 前言  
  - 教材：周志华《机器学习》
  - 期末考试：6.10 下午，闭卷笔试
  - 作业包括纸面作业和上机作业，都在教学网进行
  - 作业/考勤 40% + 期中 20% + 期末 40%
  - 教学大纲：
    + 机器学习概论
    + 计算学习理论
    + 机器学习基本模型和算法
    + 深度学习初步
= 基本概念
  == 监督学习
    所谓监督学习，就是基于一组数据 $T = {(x_i, y_i)}$ 来学习输入空间 $X$ 到输出空间 $Y$ 的映射。通常认为样本是独立同分布的。$y_i = c(x_i)$ 有时（$c$）也称为目标概念。所有概念记为 $C$，而所有可能的概念一般记为假设空间 $H$，往往与 $C$ 不同。学习算法基于 $T$ 得到一个假设 $h_T in H$
    #definition[损失函数][
      用 $L(h(x), y)$ 度量以 $h(x)$ 作为预测的预测好坏，常见的包括：
      - 0-1 损失
        $
          L(h(x), y) = delta_(h(x), y)
        $ 
      - 平方损失
        $
          L(h(x), y) = (h(x) - y)^2
        $
    ]
    #definition[泛化误差][
      给定损失函数，则：
      $
        ge(h) = E_(x tilde D) L(h(x), c(x))
      $
      表明平均意义下预测的好坏，称其为泛化误差
    ]
    注意通常来说，$x$ 的分布 $D$ 和真实概念 $c$ 都是未知的，因此泛化误差通常是无法实际计算的。
    #definition[经验误差/测试误差][
      - 给定数据集 $T$，则：
        $
          ee(H) = E_T L(h(x), y_i) = 1/N sum_i L(h(x_i), y_i)
        $
        称为经验误差。经验误差小未必保证泛化误差小，可能产生*过拟合*现象。

        更进一步，不同的机器学习理论可能给出对：
        $
          P(abs(approxVar(R)(h_T) - R(h_T)) < epsilon) 
        $
        的估计
      - 类似的，在测试集上计算的误差称为测试误差。通常来说，测试集的产生与训练过程无关，可以认为它们独立，同时有：
        $
          E (approxVar(R)_"test" (h_T)) = R(h_T) 
        $

    ]
    #definition[误差率/准确率][
      用：
      $
        er(h_T) = E_T delta_(h(x), y_i)
      $
      表示算法在 $T$ 上的误差率。用：
      $
        acc(h_T) = 1 - er(h_T)
      $
      表示算法在 $T$ 上的准确率。
    ]
    经验误差小并不能保证泛化误差组功效。
    
    实际二分类问题中，往往会将结果分为真/T 以及假/F，因此产生四类情况：
    - TP：真正例
    - FP：假正例
    - TN：真负例
    - FN：假负例
    通常：
    - $"TP"/("TF" + "TP")$ 称为准确率/查准率
    - $"TP"/("TP" + "FN")$ 称为召回率/查全率

    实际训练模型中，通常来说有两种策略：
    - 经验风险最小化：最小化经验误差
    - 结构风险最小化/正则化策略：
      $
        argmin_h approxVar(R) (h) + lambda J(h)
      $
      其中 $J(h)$ 为正则化项，$lambda$ 为正则化参数。实践上，这两者的选择方法是十分复杂的。
    
    实践上由于超参数的存在，数据往往还要分出一部分用以验证，具体来说：
    - 选取不同的超参数，在训练集上训练
    - 在验证集上验证，选取最优的超参数
    - 在测试集上测试，得到最终的对模型的评价
    #algorithm[数据集的划分][
      - 留出法/简单交叉验证法：通过随机采样，将数据划分为训练集和测试集。有时可以采用分层采样的方式，保证数据的分布。
      - $k$ 值交叉验证法：将数据划分为 $k$ 个大小相等的子集，每次取一个子集作为测试集，其余的作为训练集，重复 $k$ 次，得到 $k$ 个模型，取平均值。
      - 留一法：$k = N$ 的特殊情况，每次只取一个样本作为测试集，其余的作为训练集。
      - 自助法：通过有放回的采样，得到一定大小的训练集，剩余的作为测试集。这样采样可以保证验证集，训练集的大小相同，但得到的分布往往和原分布有所不同。
    ]

    #definition[][
      - 定义：
        $
          overline(h) (x) = E_T h_T (x)\
          "Bias" (x) = E_T (h_T (x) - c(x)) \
          "Var" (x) = E_T (h_T (x) - overline(h) (x))^2
        $
    ]
    #lemma[][
      $
        E_Y (h_T (x) - c(x))^2 = "Bias" (x)^2 + "Var" (x)
      $
    ]
    然而，实际上偏差小（表示对数据敏感）和方差小（对数据不敏感）往往是矛盾的。
= 支持向量机
  以二分类任务为例，使用最简单的线性模型，就是找一个超平面将数据分开：
  - 若假设存在这样一个超平面，如何找到？
  - 若不假设，如何找到一个最优的超平面？
  == 线性可分支持向量机
    给定一个线性可分的数据集 $T = {(x_i, y_i)}$，其中 $y_i in {1, -1}$，则存在一个超平面 $w^T x + b = 0$，使得对所有的 $i$ 有 $y_i (w^T x_i + b) > 0$。

    若模型正确分类，则一定有：
    $
      y_i (w^T x_i + b) > 0, forall i
    $
    同时，希望找到最好的超平面，也即：
    $
      max_(w, b) 1/(norm(w)) suchThat y_i (w^T x_i + b) > 0, forall i 
    $
    其中 $1/(norm(w))$ 为间隔，表示超平面到最近的点的距离。它可以转化为：
    $
      min_(w, b) 1/2 norm(w)^2 suchThat y_i (w^T x_i + b) > 0, forall i
    $
    这是凸二次优化问题，只要有解（原问题线性可分），则解唯一。称它的解为最大间隔分离超平面。某种意义上，就是利用下面的神经元：
    $
      f(x) = sgn(w^T x + b)
    $
    为了求解这个问题，构造拉格朗日函数：
    $
      L = 1/2 norm(w)^2 - sum_(i = 1)^N alpha_i - sum_(i = 1)^N alpha_i y_i (w^T x_i + b)
    $
    可得：
    $
      W = sum_i alpha_i y_i x_i\
      sum_i alpha_i y_i = 0
    $
    显然 $W != 0$，因此 $alpha_i$ 不能全为零。由 KKT 条件，若 $alpha_i != 0$，则：
    $
      y_j (w^T x_j + b) = 1\
      b = y_j - w x_j
    $
    由上面的推导，往往也将支持向量机写作：
    $
      sgn(sum_i alpha_i y_i x_i^T x + b)
    $
    事实上，还可以得到：
    $
      norm(W)^2 = sum_i alpha_i
    $
    因此，事实上我们希望不为零的 $alpha_i$ 尽可能少，这些向量就被称为支持向量
  == 非线性可分支持向量机
